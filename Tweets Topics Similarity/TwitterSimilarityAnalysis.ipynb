{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy \n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwitterClient(): \n",
    "    # keys and tokens from the Twitter Dev Console \n",
    "    api_key = \"xQGrEytN0KSsEERUlbA3Mwjdv\"\n",
    "    api_secret = \"N7ModRAyQqtFkB4jvI1o0nCooyfSGfZZUAaIfHTQp3iiUGutKi\"\n",
    "    access = \"3118781631-GnQENiXAMouhIuhas2ol14A4b9zyChTR927WZmW\"\n",
    "    access_secret = \"frEmYjSFnETQka3sOtTb2x6oCOAkFHDmSivcgrqULytRL\"\n",
    "\n",
    "    # attempt authentication \n",
    "    try: \n",
    "        # create OAuthHandler object \n",
    "        auth = OAuthHandler(api_key, api_secret) \n",
    "        #set access token and secret \n",
    "        auth.set_access_token(access, access_secret) \n",
    "        # create tweepy API object to fetch tweets \n",
    "        api = tweepy.API(auth) \n",
    "    except: \n",
    "        print(\"Error: Authentication Failed\") \n",
    "\n",
    "    return api #now we can make request to twitter using this api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = TwitterClient()\n",
    "\n",
    "# get tweets from a user and add the tweets to a dataframe\n",
    "def get_tweets(user_name):\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Cursor(api.user_timeline, id=user_name).items():\n",
    "        tweets.append(tweet)\n",
    "    df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeff_tweets = get_tweets('JeffBezos')\n",
    "jeff_tweets.columns = ['Jeff Bezos']\n",
    "\n",
    "elon_tweets = get_tweets('elonmusk')\n",
    "elon_tweets.columns = ['Elon Musk']\n",
    "\n",
    "snoop_tweets = get_tweets('SnoopDogg')\n",
    "snoop_tweets.columns = [\"Snoop Dogg\"]\n",
    "\n",
    "taylor_tweets = get_tweets('taylorswift13')\n",
    "taylor_tweets.columns = [\"Taylor Swift\"]\n",
    "\n",
    "slash_tweets = get_tweets('Slash')\n",
    "slash_tweets.columns = [\"Slash\"]\n",
    "\n",
    "wiz_tweets = get_tweets('wizkhalifa')\n",
    "wiz_tweets.columns = [\"Wiz Khalifa\"]\n",
    "\n",
    "nat_geo_tweets = get_tweets('natGeo')\n",
    "nat_geo_tweets.columns = [\"National Geographic\"]\n",
    "\n",
    "bbc_earth_tweets = get_tweets('BBCEarth')\n",
    "bbc_earth_tweets.columns = [\"BBC Earth\"]\n",
    "\n",
    "biden_tweets = get_tweets('JoeBiden')\n",
    "biden_tweets.columns = [\"Joe Biden\"]\n",
    "\n",
    "nasa_tweets = get_tweets('NASA')\n",
    "nasa_tweets.columns = [\"NASA\"]\n",
    "\n",
    "nba_tweets = get_tweets('NBA')\n",
    "nba_tweets.columns = [\"NBA\"]\n",
    "\n",
    "ronaldo_tweets = get_tweets('Cristiano')\n",
    "ronaldo_tweets.columns = [\"Cristiano Ronaldo\"]\n",
    "\n",
    "neymar_tweets = get_tweets('neymarjr')\n",
    "neymar_tweets.columns = [\"Neymar Jr\"]\n",
    "\n",
    "lexFridman_tweets = get_tweets('lexfridman')\n",
    "lexFridman_tweets.columns = [\"Lex Fridman\"]\n",
    "\n",
    "tesla_tweets = get_tweets('Tesla')\n",
    "tesla_tweets.columns = [\"Tesla\"]\n",
    "\n",
    "dogecoin_tweets = get_tweets('dogecoin')\n",
    "dogecoin_tweets.columns = [\"Dogecoin\"]\n",
    "\n",
    "jackDorsey_tweets = get_tweets('jack')\n",
    "jackDorsey_tweets.columns = [\"Jack Dorsey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"Jeff Bezos\"] = jeff_tweets[\"Jeff Bezos\"]\n",
    "df[\"Elon Musk\"] = elon_tweets[\"Elon Musk\"]\n",
    "df[\"Snoop Dogg\"] = snoop_tweets[\"Snoop Dogg\"]\n",
    "df[\"Taylor Swift\"] = taylor_tweets[\"Taylor Swift\"]\n",
    "df[\"Slash\"] = slash_tweets[\"Slash\"]\n",
    "df[\"Wiz Khalifa\"] = wiz_tweets[\"Wiz Khalifa\"]\n",
    "df[\"National Geographic\"] = nat_geo_tweets[\"National Geographic\"]\n",
    "df[\"BBC Earth\"] = bbc_earth_tweets[\"BBC Earth\"]\n",
    "df[\"Joe Biden\"] = biden_tweets[\"Joe Biden\"]\n",
    "df[\"NASA\"] = nasa_tweets[\"NASA\"]\n",
    "df[\"NBA\"] = nba_tweets[\"NBA\"]\n",
    "df[\"Cristiano Ronaldo\"] = ronaldo_tweets[\"Cristiano Ronaldo\"]\n",
    "df[\"Neymar Jr\"] = neymar_tweets[\"Neymar Jr\"]\n",
    "df[\"Lex Fridman\"] = lexFridman_tweets[\"Lex Fridman\"]\n",
    "df[\"Tesla\"] = tesla_tweets[\"Tesla\"]\n",
    "df[\"Dogecoin\"] = dogecoin_tweets[\"Dogecoin\"]\n",
    "df[\"Jack Dorsey\"] = jackDorsey_tweets[\"Jack Dorsey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "\n",
    "# convert to lower case\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# remove stop words\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update([\"https\", \"co\", \"http\", \"'\", \"rt\", \"nan\"])\n",
    "    word_tokens = text.split()\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "# call all 3 functions and change the dataframe in a for loop\n",
    "for i in range(len(df)):\n",
    "    for name in columns:\n",
    "        df[name][i] = remove_punctuation(df[name][i])\n",
    "        df[name][i] = lower_case(df[name][i])\n",
    "        df[name][i] = remove_stop_words(df[name][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings = {}\n",
    "for name in columns:\n",
    "    df_strings[name] = \" \".join([str(word) for item in df[name] for word in item])\n",
    "df_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wordcloud for each name and plot all of them in a grid\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, name in enumerate(columns):\n",
    "    # create a word cloud for each name\n",
    "    plt.subplot(6, 3, i+1)\n",
    "    wordcloud = WordCloud(background_color='white', max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "    wordcloud.generate(df_strings[name])\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, name in enumerate(columns):\n",
    "    # create a word cloud for each name\n",
    "    plt.subplot(6, 3, i+1)\n",
    "    word_counts = pd.Series(df_strings[name].split()).value_counts()\n",
    "    word_counts = word_counts[:5]\n",
    "    word_counts.plot.bar()\n",
    "    plt.title(name)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\"https\", \"co\", \"http\", \"'\", \"rt\", \"nan\", \"k\", \"n\", \"u\", \"iiii\", \"new\", \"e\", \"se\", \"one\", \"amp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all the strings in columns and create a wordcloud\n",
    "df_strings_joined = \" \".join([str(word) for item in df_strings.values() for word in item])\n",
    "wordcloud = WordCloud(background_color='white', max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(df_strings_joined)\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "word_counts = pd.Series(df_strings_joined.split()).value_counts()\n",
    "word_counts = word_counts[:26]\n",
    "word_counts.plot.bar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update(word_counts.index[:26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "\n",
    "# documents = list of each df_string\n",
    "documents = []\n",
    "for name in columns:\n",
    "    documents.append(df_strings[name])\n",
    "\n",
    "texts = [[word.lower() for word in document.split() if word.lower() not in stop_words] for document in documents]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]   #vector representation of each document\n",
    "\n",
    "# Latent semantic indexing (also referred to as Latent Semantic Analysis) is a method of analyzing \n",
    "# a set of documents in order to discover statistical co-occurrences of words that appear together\n",
    "# which then give insights into the topics of those words and documents.\n",
    "# It does it thanks to a mathematical technique called singular value decomposition (SVD).\n",
    "# The LSI model is a Latent Semantic Indexing model that is based on a matrix decomposition of the\n",
    "# document-term matrix.\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"crypto\"\n",
    "\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "\n",
    "# convert the query to LSI space\n",
    "vec_lsi = lsi[vec_bow]\n",
    "\n",
    "#Compute similarity against a corpus of documents by storing the index matrix in memory. \n",
    "#The similarity measure used is cosine between two vectors.\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "\n",
    "# perform a similarity query against the corpus\n",
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "#print sims but the names of the indexes and the points\n",
    "for i in range(len(sims)):\n",
    "    print(columns[sims[i][0]], sims[i][1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f44ca2975e2b24df1b43f43147d75ff5945d4dff92c19966ebf2b71fce0f9e9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DataScience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
